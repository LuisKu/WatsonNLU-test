{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and Analytics workshop - Using Natural Language Understanding and Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this portion of the workshop, we'll use an instance of [Watson Natural Language Understanding](https://cloud.ibm.com/catalog/services/natural-language-understanding) to gather insights into data.\n",
    "\n",
    "Watson Natural Language Understanding is a cloud native product that uses deep learning to extract metadata from text such as entities, keywords, categories, sentiment, emotion, relations, and syntax.\n",
    "There is a rich [API](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python) that we will use along with the [Watson Python SDK](https://github.com/watson-developer-cloud/python-sdk) to analyze our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [1.0 Setup - install modules](#setup)\n",
    "- [2.0 Test NLU APIs](#test)\n",
    "- [3.0 Import Data and Setup Pandas Dataframe ](#pandas)\n",
    "- [4.0 Clean and Prepare data for NLU scoring](#clean)\n",
    "- [5.0 Analyze response from NLU ](#analyze)\n",
    "- [6.0 Get sentiment by row](#sentiment-row)\n",
    "- [7.0 Graph with matplotlib](#graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Setup - Install Modules<a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [Watson Python SDK](https://github.com/watson-developer-cloud/python-sdk) to access the [NLU APIs](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python) programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.18.5 in c:\\users\\luisk\\desktop\\sample_project_1\\env\\lib\\site-packages (1.18.5)\n",
      "Requirement already satisfied: pandas==1.0.5 in c:\\users\\luisk\\desktop\\sample_project_1\\env\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\luisk\\desktop\\sample_project_1\\env\\lib\\site-packages (from pandas==1.0.5) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\luisk\\desktop\\sample_project_1\\env\\lib\\site-packages (from pandas==1.0.5) (1.18.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\luisk\\desktop\\sample_project_1\\env\\lib\\site-packages (from pandas==1.0.5) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\luisk\\desktop\\sample_project_1\\env\\lib\\site-packages (from python-dateutil>=2.6.1->pandas==1.0.5) (1.15.0)\n",
      "Requirement already satisfied: ibm-watson==4.7.1 in c:\\users\\luisk\\desktop\\sample_project_1\\env\\lib\\site-packages (4.7.1)\n",
      "Requirement already satisfied: ibm-cloud-sdk-core==1.7.3 in c:\\users\\luisk\\desktop\\sample_project_1\\env\\lib\\site-packages (from ibm-watson==4.7.1) (1.7.3)\n",
      "Requirement already satisfied: websocket-client==0.48.0 in c:\\users\\luisk\\desktop\\sample_project_1\\env\\lib\\site-packages (from ibm-watson==4.7.1) (0.48.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\luisk\\desktop\\sample_project_1\\env\\lib\\site-packages (from ibm-watson==4.7.1) (2.8.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.0 in c:\\users\\luisk\\desktop\\sample_project_1\\env\\lib\\site-packages (from ibm-watson==4.7.1) (2.26.0)\n",
      "Requirement already satisfied: PyJWT>=1.7.1 in c:\\users\\luisk\\desktop\\sample_project_1\\env\\lib\\site-packages (from ibm-cloud-sdk-core==1.7.3->ibm-watson==4.7.1) (2.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\luisk\\desktop\\sample_project_1\\env\\lib\\site-packages (from websocket-client==0.48.0->ibm-watson==4.7.1) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\luisk\\desktop\\sample_project_1\\env\\lib\\site-packages (from requests<3.0,>=2.0->ibm-watson==4.7.1) (2020.12.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\luisk\\desktop\\sample_project_1\\env\\lib\\site-packages (from requests<3.0,>=2.0->ibm-watson==4.7.1) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\luisk\\desktop\\sample_project_1\\env\\lib\\site-packages (from requests<3.0,>=2.0->ibm-watson==4.7.1) (2.0.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\luisk\\desktop\\sample_project_1\\env\\lib\\site-packages (from requests<3.0,>=2.0->ibm-watson==4.7.1) (1.26.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy==1.18.5\n",
    "!pip install --upgrade pandas==1.0.5\n",
    "!pip install --upgrade ibm-watson==4.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Restart the Jupyter kernel now\n",
    "Restart the kernal by going to the `Kernel` tab above and choosing `Restart`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import python modules from the Watson Python SDKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson.natural_language_understanding_v1 import Features,CategoriesOptions,ConceptsOptions,SentimentOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Add NLU credentials\n",
    "Get the [IAM Authentication Key](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#authentication) and [Service URL](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#service-endpoint) that you obtained when you [Created a Watson NLU instance](https://github.ibm.com/IBMDeveloper/python-and-analytics/tree/addNLU/workshop/natural-language-understanding#create-a-watson-nlu-instance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your [IAM Authentication Key](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#authentication) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IAM_KEY = 'nQ87qJzF-1WmKI9AktrcOxQK2LuMd_i3UAhTvoEVgUWA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your [NLU Service URL](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#service-endpoint) below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_URL = 'https://api.us-south.natural-language-understanding.watson.cloud.ibm.com/instances/7eeddfba-3008-4e38-bf0a-b49cad5e6aa0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 NLU APIs <a name=\"test\"></a>\n",
    "Run a quick check to make sure everything is working. We'll use a [basic web page](https://www.ibm.com) to see how Watson Natural Language Understanding can extract categories when given a URL. [This example](https://cloud.ibm.com/apidocs/natural-language-understanding?code=python#categories) comes from the Watson NLU documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authenticator = IAMAuthenticator(IAM_KEY)\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(version='2020-08-01',authenticator=authenticator)\n",
    "\n",
    "natural_language_understanding.set_service_url(SERVICE_URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index;numero post;comentarios</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1;1;hola como estas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2;1;bien y tu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  index;numero post;comentarios\n",
       "0           1;1;hola como estas\n",
       "1                 2;1;bien y tu"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, types\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/LuisKu/WatsonNLU-test/main/df1.csv')\n",
    "df2 = pd.read_csv('https://raw.githubusercontent.com/LuisKu/WatsonNLU-test/main/df2.csv')\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Import Data and Setup Pandas Dataframe <a name=\"pandas\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read [cfpciti.csv](https://raw.githubusercontent.com/IBM/python-and-analytics/master/data/cfpbciti.csv) which contains data from the Consumer Credit Bureau for consumer complaints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, types\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "from datetime import datetime\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "\n",
    "if os.environ.get('RUNTIME_ENV_LOCATION_TYPE') == 'external':\n",
    "    endpoint_e003a95389e748a098b8990c00fade00 = 'https://s3.us.cloud-object-storage.appdomain.cloud'\n",
    "else:\n",
    "    endpoint_e003a95389e748a098b8990c00fade00 = 'https://s3.us.cloud-object-storage.appdomain.cloud'\n",
    "\n",
    "client_e003a95389e748a098b8990c00fade00 = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='nVoeoIpOxHn8PotTIYis2Whv0LbYW7XZ0isb0dgqdWQJ',\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url=endpoint_e003a95389e748a098b8990c00fade00)\n",
    "\n",
    "body = client_e003a95389e748a098b8990c00fade00.get_object(Bucket='tallerwatson-donotdelete-pr-88y2mdxxruewfv',Key='Test.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "df = pd.read_csv(body)\n",
    "df2 = pd.read_csv(body2)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Get sentiment by row <a name=\"sentiment-row\"></a>\n",
    "Now, let's derive some sentiment and emotion information on a per-row basis, to provide more granualarity.\n",
    "The number of API calls that you can make to Watson NLU is [rate limited and dependent on your service plan](https://cloud.ibm.com/catalog/services/natural-language-understanding), so in order to limit the number of API calls to the NLU endpoint we'll start with just 50 rows by setting `num_rows` to 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = df\n",
    "df2_rows = df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks like what we want. Now, we'll create a list to hold the `responses`, call Watson NLU with the data and then populate the responses list. We'll do the same with a list called `normalize` that we can use along with [json_normalize()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentComentario = []\n",
    "conceptoDescripcion = []\n",
    "\n",
    "for index, row in df_rows.iterrows():\n",
    "    \n",
    "    response = natural_language_understanding.analyze(\n",
    "    text = row['Consumer complaint narrative'],\n",
    "    features=Features(sentiment=SentimentOptions(document))).get_result()\n",
    "    sentimentComentario.append(response)\n",
    "\n",
    "for index, row in df2_rows.iterrows():\n",
    "    \n",
    "    response = natural_language_understanding.analyze(\n",
    "    text = row['Consumer complaint narrative'],\n",
    "    features=Features(concepts=ConceptsOptions(limit=1)).get_result()\n",
    "    conceptoDescripcion.append(response)",     
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the `responses` list and the `normalize` to the df_rows dataframe. We can continue to use these new data features, but more commonly we'll derive new dataframes for our experiments and change those new dataframes instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows['sentiment'] = sentimentComentario\n",
    "df2_rows['concepto'] = conceptoDescripcion\n",
    "df2_rows.head(5)\n",
    "df_rows.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new dataframe where we can pull out the column for the `emotion` `anger`, then sort by the highest rating of `anger`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
